# chatllm.rs

rust api wrapper for llm-inference chatllm.cpp

All credits go to original repo: https://github.com/foldl/chatllm.cpp and Qwen 2.5 32b Coder Instruct which made 99% of work. I only guided it with prompts.

To compile your project into an executable, open a terminal and navigate to your project directory. Then, run the following command:

cargo build --release


